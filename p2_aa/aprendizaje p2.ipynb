{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd ; import numpy as np ; import statsmodels.api as sm\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "data_train=pd.read_csv('./dataset_housing_preprocesado_train.csv')\n",
    "data_test=pd.read_csv('./dataset_housing_preprocesado_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 781 entries, 0 to 780\n",
      "Columns: 153 entries, LotFrontage to Foundation_PConc\n",
      "dtypes: float64(2), int64(151)\n",
      "memory usage: 933.7 KB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(data_train['Exterior_CBlock'])  #borramos la columna de ceros que no nos dejaba hacer la matriz de covarianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procedemos a eliminar aquellas variables colineales, las detectaremos al ver en la matriz de correlación valores superiores a | 0.8 |, ya que nos ha parecido un valor óptimo para ver las colineales. Tiene que ser valor absoluto ya que pueder ser colineales y tener correlación negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 781 entries, 0 to 780\n",
      "Columns: 152 entries, LotFrontage to Foundation_PConc\n",
      "dtypes: float64(2), int64(150)\n",
      "memory usage: 927.6 KB\n"
     ]
    }
   ],
   "source": [
    "matriz_cor=data_train.corr()\n",
    "data_train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LotFrontage', 'LotArea', 'LotShape', 'LandContour', 'LotConfig',\n",
      "       'LandSlope', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
      "       ...\n",
      "       'HouseStyle_SFoyer', 'HouseStyle_SLvl', 'RoofStyle_Hip',\n",
      "       'RoofStyle_Other', 'MasVnrType_BrkFace', 'MasVnrType_None',\n",
      "       'MasVnrType_Stone', 'Foundation_CBlock', 'Foundation_Other',\n",
      "       'Foundation_PConc'],\n",
      "      dtype='object', length=140)\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "data_train_aux = data_train\n",
    "cols = data_train.columns\n",
    "rank = np.linalg.matrix_rank(data_train.cov())\n",
    "\n",
    "for col in cols:\n",
    "    cols2 = cols.drop(col)\n",
    "    data_train_aux = data_train_aux[cols2]\n",
    "    rank2 = np.linalg.matrix_rank(data_train_aux.cov())\n",
    "    if rank == rank2:\n",
    "        cols = cols2 #si el rango es igual la columna era linealmente dep\n",
    "    else:\n",
    "        data_train_aux = data_train #si no habremos bajado un rango porq era indep\n",
    "\n",
    "\n",
    "data_train = data_train[cols]\n",
    "print(data_train.columns)\n",
    "print(np.linalg.matrix_rank(data_train.cov()))\n",
    "\n",
    "matriz_cor=data_train.cov()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividimos los datos entre la variable objetivo/dependiente (y) y las variables independientes\n",
    "x_train = data_train.drop('SalePrice', axis=1)\n",
    "y_train = data_train['SalePrice']\n",
    "x_test  = data_test.drop(\"SalePrice\", axis=1)\n",
    "y_test  = data_test[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEMANA 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 1: Construcción de un modelo lineal con librería statsmodels\n",
    "\n",
    "\n",
    "• Construye un modelo de regresión para el dataset anterior usando la libreríastatsmodels. ¿Obtiene los mismos coeficientes que sklearn? ¿Los coeficientes de determinación son iguales? ¿Es el QQ-plot el esperado para un modelo de regresión lineal?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al construir el modelo de regresión lineal con statsmodels miramos que el coeficiente de determinación de model es del 0.991, por lo que vemos innecesario hacer una selección de variables. Esto también tiene que ver con el hecho de que nuestros datos ya han pasado por un preporocesado anterior. Si esto no fuese así, buscaríamos las variables cuyo p-valor fuese mayor que el nivel de significación elegido y las quitaríamos del modelo para tener un mayor coeficiente R2.\n",
    "\n",
    "Los coeficientes son similares a sklearn son similares con valores absolutos pequeños, pero cuánto más grandes son estos más error puede llegar a ver entre ellos. Por ejemplo, los coeficientes de statsmodels y skelarn en LotShape son 1136 y 674.55,repectivamente. \n",
    "\n",
    "Como al ejecutar el QQPlot se visualiza el trazado ascendente de los residuos ordenados que conforman una clara línea, podemos confirmar que es la gráfica que esperábamos para el modelo de regresión lineal. La única pega son resiudos más altos, que se alejan de la recta. Esto quiere decir que la regresión lineal no se ajusta tanto para estos valores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=sm.add_constant(x_train)\n",
    "model = sm.OLS(y_train, x_train).fit()\n",
    "\n",
    "\n",
    "display(model.summary())\n",
    "\n",
    "    \n",
    "sm.qqplot(model.resid, line='s')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Saca por pantalla la información estadística del modelo: desviación estándar de los coeficientes, valores p, intervalos de confianza de los coeficientes. Con una significancia estadística del 95%, ¿qué variables parece que no aportan\n",
    "información?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 2: Selección de variables con statsmodels\n",
    "\n",
    "• Implementa el siguiente algoritmo de eliminación de variables usando\n",
    "statsmodels:\n",
    "1- Construye modelo con variables cols\n",
    "2- Detecta qué variables parece que no aportan información de acuerdo al\n",
    "criterio de los valores P. Si no encuentras ninguna, fin del algoritmo. Si\n",
    "encuentras una o varias, toma la que tenga un valor P mayor y quítala de\n",
    "cols. Vuelve a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• ¿Con cuántas variables te quedas usando este algoritmo? ¿Qué R2 tiene en training y en test el nuevo modelo? ¿Es mejor que el modelo sin seleccionar variables?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 3: Selección de variables con regularización\n",
    "\n",
    "• Crea un modelo lineal usando regresión Lasso usando la clase LassoCV del módulo linear_model de sklearn. ¿Qué variables tienen peso 0 en el modelo construido? ¿es esto consistente con lo que has visto en el paso 1? ¿Qué R2 en training y test tiene tu modelo? ¿es mejor respecto a los anteriores?\n",
    " Importante: para usar regresión Lasso o Ridge, las variables de entrada al modelo y la variable target se deben estandarizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Repite el punto anterior usando regresión Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
